# Fraud Detection API (FastAPI + Spark Training + sklearn Inference)

Productionâ€‘ready fraud detection service built with **PySpark for feature engineering & training** and **FastAPI + sklearn for lightweight cloud inference**.

This project demonstrates how to move from a distributed ML training pipeline to a deployable, lowâ€‘latency prediction API suitable for cloud platforms like Render.

---

#  Overview

This system detects potentially fraudulent transactions using engineered behavioral and merchantâ€‘risk features.

Training is performed with **Spark ML** on parquet feature data, while inference is served using a **sklearn Gradient Boosting model** and a manually exported scaler â€” avoiding Spark at runtime for reliability and speed.

---

#  ML Pipeline Design

## Training (Spark)

* Load parquet transaction feature dataset
* Feature assembly with Spark VectorAssembler
* Standard scaling with Spark StandardScaler
* Train models:

  * Logistic Regression
  * Gradient Boosted Trees (Spark)
* Export scaler statistics (mean/std) â†’ JSON
* Sample data â†’ convert to Pandas â†’ train sklearn GBT
* Save sklearn model for inference

## Inference (API)

* FastAPI REST service
* Manual feature vector builder
* Manual standard scaling using exported scaler stats
* sklearn GradientBoostingClassifier for prediction
* Probability + binary fraud flag returned

This avoids JVM/Spark dependencies in production.

---

# ğŸ—‚ Project Structure

```
pip install -r requirements.txtFRAUD-DETECTION-PIPELINE/
â”‚
â”œâ”€â”€ models_a/
â”‚   â”œâ”€â”€ gbt_sklearn.joblib      # âœ… Production model (sklearn) used by API
â”‚   â”œâ”€â”€ scaler.json             # âœ… Feature scaling parameters used by API
â”‚   â”‚
â”‚   â”œâ”€â”€ assembler/              # âš ï¸ Spark VectorAssembler (training artifact)
â”‚   â”œâ”€â”€ scaler/                 # âš ï¸ Spark StandardScalerModel (training artifact)
â”‚   â”œâ”€â”€ gbt_fraud_model/        # âš ï¸ Spark GBT model (training artifact)
â”‚   â””â”€â”€ lr_fraud_model/         # âš ï¸ Spark Logistic Regression model (training artifact)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # âœ… FastAPI app â€” API endpoints (/health, /predict)
â”‚   â”œâ”€â”€ model.py                # âœ… Loads sklearn model for inference
â”‚   â”œâ”€â”€ features.py             # âœ… Builds + scales feature vector from request
â”‚   â”‚
â”‚   â”œâ”€â”€ training/               # âš ï¸ Spark model training pipeline
â”‚   â”œâ”€â”€ ingestion/              # âš ï¸ Data ingestion logic
â”‚   â”œâ”€â”€ features/               # âš ï¸ Spark feature engineering modules
â”‚   â”œâ”€â”€ inference/              # ğŸ—‘ Legacy Spark inference code (not used now)
â”‚   â”œâ”€â”€ data/                   # âš ï¸ Training datasets / feature tables
â”‚   â””â”€â”€ __pycache__/            # ğŸ—‘ Python cache
â”‚
â”œâ”€â”€ Dockerfile                  # âœ… Render deployment container config
â”œâ”€â”€ requirements.txt            # âœ… Dependencies for API runtime
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE

```

---

# This app is live at https://fraud-detection-pipeline-1-uag0.onrender.com 

feel free to check it out :)
to test out the predict end point 
https://fraud-detection-pipeline-1-uag0.onrender.com/docs 


# Installation (Local)

## 1ï¸âƒ£ Clone repo

```bash
git clone <this-repo-url>
cd fraud-detection-pipeline
```

## 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

## 3ï¸âƒ£ Run API

```bash
uvicorn src.main:app --reload
```

---

#  API Usage

## Health Check

```
GET /health
```

Response:

```json
{"status": "ok"}
```

---

## Prediction

```
POST /predict
```

### Example Request

```json
{
  "tx_count_user_1h": 2,
  "amount_vs_user_avg_24h": 1.3,
  "avg_amount_user_24h": 120,
  "amount_vs_merchant_avg": 0.9,
  "tx_count_merchant_1h": 4,
  "merchant_velocity_spike": 0,
  "merchant_fraud_rate": 0.01,
  "amount_log": 4.2,
  "is_high_amount": 1,
  "foreign_high_amount": 0,
  "high_amount_mobile": 1,
  "repeat_user_fast": 0,
  "merchant_risky_high_amount": 0
}
```

### Example Response

```json
{
  "fraud_probability": 0.73,
  "is_fraud": 1
}
```

---

#  Deployment (Render)

Create a **Render Web Service** connected to this repo.

### Build Command
   ```
pip install -r requirements.txt
```

### Start Command

```
uvicorn src.main:app --host 0.0.0.0 --port 10000
```

After deployment:

```
https://<service>.onrender.com/docs
```

---

#  Why sklearn for Inference Instead of Spark

Running Spark inside web APIs causes:

* JVM memory pressure
* container crashes
* token/accumulator errors
* cold start delays

Using sklearn for inference gives:

âœ… fast startup
âœ… low memory usage
âœ… cloud compatibility
âœ… simpler deployment

Industry standard pattern: **Distributed training â†’ lightweight inference service**.

---

# ğŸ“Š Features Used

* User transaction velocity
* Merchant velocity spikes
* Relative transaction amount
* Merchant fraud rate
* Highâ€‘amount flags
* Behavioral repeat patterns

All features are standardized using trainingâ€‘time statistics.

---

#  Future Improvements

* API key authentication
* Rate limiting
* Batch prediction endpoint
* Model versioning
* Monitoring & drift detection
* Feature store integration


#  License

MIT License
